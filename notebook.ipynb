{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matheus-de-araujo/fakenews-classifier-notebook/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZEJTisnX3UH"
      },
      "source": [
        "# 1. Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Zz7uhiFbXkmW"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import string\n",
        "import os, re\n",
        "\n",
        "# Nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import RSLPStemmer\n",
        "import nltk\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h7uvesXYaJ9"
      },
      "source": [
        "# 2. Realizando a leitura do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yAki4irkXkme",
        "outputId": "e2fd2990-1bdf-4915-98d5-b389bf172ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A pasta já existe!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists('dataset-fake-br-for-tcc/'):\n",
        "    print('A pasta já existe!')\n",
        "else:\n",
        "  !git clone https://github.com/matheus-de-araujo/dataset-fake-br-for-tcc.git\n",
        "\n",
        "def returnDataSetFrame():\n",
        "    dataFrame = pd.DataFrame(columns=[ 'text', 'raw', 'label'])\n",
        "    paths = ['dataset-fake-br-for-tcc/full_texts/fake/', 'dataset-fake-br-for-tcc/full_texts/true/']\n",
        "\n",
        "    for filename in os.listdir(paths[0]):\n",
        "        with open(paths[0]+filename, 'r') as f:\n",
        "            text = f.read()\n",
        "            dataFrame.loc[len(dataFrame)] = [preProcessing(text, forCloudOfWords = False), text, 'fake']\n",
        "\n",
        "    for filename in os.listdir(paths[1]):\n",
        "        with open(paths[1]+filename, 'r') as f:\n",
        "            text = f.read()\n",
        "            dataFrame.loc[len(dataFrame)] = [preProcessing(text, forCloudOfWords = False), text, 'true']\n",
        "    return dataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBZrvNwBQy6"
      },
      "source": [
        "# 3. Funções para limpeza dos dados e remoção dos StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "itPlQXF0Xkmo",
        "outputId": "ba616dbb-17b1-4d28-bd2c-017175fa3c73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def cleaning_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(\"(\\\\d|\\\\W|\\d+\\w+|\\\\n)+\",\" \",sentence)\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    return sentence\n",
        "\n",
        "def Stemming(sentence):\n",
        "    stemmer = RSLPStemmer()\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        phrase.append(stemmer.stem(word.lower()))\n",
        "    return phrase\n",
        "\n",
        "def removeStopWords(sentence):\n",
        "    stopwords_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "    sentence = ' '.join([word for word in sentence.split() if word not in stopwords_nltk])\n",
        "    return sentence\n",
        "\n",
        "def preProcessing(sentence, forCloudOfWords = True):\n",
        "    if (forCloudOfWords):\n",
        "        return removeStopWords(cleaning_sentence(sentence))\n",
        "    return Stemming(removeStopWords(cleaning_sentence(sentence)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZgNbp0uGOO0"
      },
      "source": [
        "# 4. Obténdo o dataset e guardando em data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMgcy5G4Xkmu"
      },
      "outputs": [],
      "source": [
        "data_text = returnDataSetFrame()\n",
        "data_text.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Funções para plotar nuvem de palavras"
      ],
      "metadata": {
        "id": "xzusycWQMo4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bagOfWords(text):\n",
        "    words = text.split()\n",
        "    counter = Counter(words) \n",
        "    return counter.most_common()\n",
        "    \n",
        "def cloudOfWords(text, n = 50,title='Bag of Words'):\n",
        "    word_freq = dict((x, y) for x, y in bagOfWords(text)[:n])\n",
        "    wordcloud = WordCloud(background_color=\"white\", width=800, height=400)\n",
        "    wordcloud.generate_from_frequencies(word_freq)\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PIk_3wluK9be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw_true = ' '.join(data_text[data_text.label == 'true']['raw'])\n",
        "data_raw_fake = ' '.join(data_text[data_text.label == 'fake']['raw'])\n",
        "\n",
        "cloudOfWords(text=data_raw_true, title='Notícias Verdadeiras (sem limpeza)')\n",
        "cloudOfWords(text=data_raw_fake, title='Notícias Falsas (sem limpeza)')"
      ],
      "metadata": {
        "id": "UGS9fm2BM5qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloudOfWords(text=preProcessing(data_raw_true), title='Notícias Verdadeiras (com limpeza)')\n",
        "cloudOfWords(text=preProcessing(data_raw_fake), title='Notícias Falsas (com limpeza)')"
      ],
      "metadata": {
        "id": "jicz224Y2Jkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Plotando o gráfico que demonstra número de ocorrência"
      ],
      "metadata": {
        "id": "WAAuYctd1E_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Número total de palavras: \" + str(data_text['text'].apply(lambda x: len(x.split(' '))).sum()))\n",
        "\n",
        "label_plot = data_text['label'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(y=label_plot.values, x=label_plot.index, alpha=0.8)\n",
        "plt.ylabel('Número de Ocorrência', fontsize=12)\n",
        "plt.xlabel('', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "cbAaHP2COTRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.85)\n",
        "\n",
        "data = data_text.sample(frac=1)\n",
        "text = data['text']\n",
        "label = data['label'].rank(method='dense', ascending=False).astype(int)\n",
        "\n",
        "data_train = vectorizer.fit_transform(text)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(text, label, test_size=0.33, random_state=42)\n",
        "\n",
        "x_train = vectorizer.fit_transform(x_train)\n",
        "x_test = vectorizer.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "MaEjJoe3OpT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "UhkJeUJM16-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_classifier = MLPClassifier(hidden_layer_sizes=(5,12),activation='tanh',solver='lbfgs',random_state=1)\n",
        "scores_neural = cross_val_score(neural_classifier, x_train, y_train, cv=10)\n",
        "Y_pred = cross_val_predict(neural_classifier,x_test,y_test, cv=10)\n",
        "conf_mat = confusion_matrix(y_test,Y_pred)\n",
        "print(\"---- Scores ----\")\n",
        "print(scores_neural)\n",
        "print(classification_report(y_test, Y_pred))\n",
        "plot_confusion_matrix(conf_mat, classes=[1, 2],title='Confusion matrix')"
      ],
      "metadata": {
        "id": "x0xLIvQvOu_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE86sE9iXkm5"
      },
      "outputs": [],
      "source": [
        "pickle.dump(vectorizer,open('model.pkl','wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}